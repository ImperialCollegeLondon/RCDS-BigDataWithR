---
title: "Part 2: More advanced big data integration with Spark"
output: html_notebook
---

# Sparkly

* Some general guidelines: If the size of data is less than 25% of computer RAM, R should generally be comfortable to process. When the size is about 50% of RAM, `data.table` should be still fine. When the size is about 50% -- 75% of RAM, it is worth trying `sparklyr`.

## Set up running environment

To set up a running environment for allowing Spark connection to a file, `sparklyr` library needs to be loaded and configured.
### Download sparklyr package

```{r check working dir}
getwd()
```


```{bash, message=F}
wget -O spark-3.5.3-bin-hadoop3.tgz https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
```


Run the following configuration to complete setup.

```{r load libs, warning=F, message=F}
library(sparklyr)
library(dplyr)
library(ggplot2)
```

```{r install sparklyr}
spark_install_tar('spark-3.5.3-bin-hadoop3.tgz')
```


```{r set up spark env}

# Initialize configuration with default settings
config <- spark_config()

# Setup memory
config$`sparklyr.shell.driver-memory` <- "4G"
config$`sparklyr.shell.executor-memory` <- "8G"
config$`spark.yarn.executor.memoryOverhead` <- "4G"

# Connnect to local cluster with the configuration given above
sc <- spark_connect(master = "local", config = config)
```
* For more details please check [here](https://therinspark.com/tuning.html).

**General Guidelines**:

Leave Memory for the OS: Reserve around 2-4GB for the operating system and other applications.

Allocate Memory for Driver (shell.driver-memory) and Executors (shell.executor-memory): Divide the remaining memory between the driver and executors.

Example Configuration:
Assuming you have 16GB RAM in total, reserve 4GB for the OS, and leave 12GB available for Spark. You can then allocate memory as follows:

Driver Memory: Allocate around 4GB to the driver.
Executor Memory: Allocate the remaining 8GB to the executors.

Optional: the yarn.executor.memoryOverhead configuration in Spark specifies the amount of additional memory allocated per executor process. 


## Set connection to the CSV data


```{bash, message=F}
# if the data doesn't exist in the current working directory
# "/workspaces/RCDS-BigDataWithR/code" (default if you haven't changed it)
# please run the following line command
unzip ../data/compustat2023.csv.zip
```


```{r connect to csv}
compustat <- spark_read_csv(sc, "computstat_gbr_2014_2024.csv")
```


## Sparklyr exercises

### Exercise 2.1 Check data 

* Find the number of rows of data
  
```{r find the row number}
sdf_nrow(compustat)
```

* Preview data
  
```{r preview}
glimpse(compustat)
```

### Exercise 2.2 Select a subset

* Select a subject contains BRITISH AMER TOBACCO PLC with iid as 01W using pipe operator.


```{r}

```

* Plot the close price "prccd" of the stock of British American Tobacco using `ggplot`.

```{r}

```

### Exercise 2.3 Add new column(s)

* Add a new column `YearQ` for holding year and quarters information. (Hint: use pipe operators and `mutate`)

```{r added year quarter}

```


### Exercise 2.4 Calculate simple daily returns.

* Use the following equation to calculate simple daily returns.

$$returns_t = \frac{prccd_t}{prccd_{t-1}} - 1$$.

```{r daily returns}

```

### Exercise 2.5 Summarise

* Summarise the mean values and SD of close price (`prccd`).

```{r}

```

### Exercise 2.6 Outlier detection

* Use `quantile` to find outliers in the daily returns.

```{r outliers detection}
# 1. Calculate quartile based on daily returns of BAT
quantile_stats 
  

# Check collected stats
print(quantile_stats)

#  2. Calculate IQR and bounds
q1 
q3 
iqr_val <- q3 - q1
lower_bound <- as.numeric(q1 - 1.5 * iqr_val)
upper_bound <- as.numeric(q3 + 1.5 * iqr_val)

print(paste("Lower Bound (IQR Method):", lower_bound))
print(paste("Upper Bound (IQR Method):", upper_bound))

# 3. Check outliers
BAT_outliers 

```


### Exercise 2.7 Clustering

* Use `ml_kmeans()` to cluster simple daily returns of "BRITISH AMER TOBACCO PLC".

```{r clustering}

# 1. select subset from "BRITISH AMER TOBACCO PLC"
 

# 2. Assemble features (K-Means needs a vector column) using `ft_vector_assembler`


# 3. K-Means (example with k=5)
kmeans_model 

results <- ml_transform(kmeans_model, returns_featured_tbl)


```

### Exercise 2.8 Clustering with multiple columns

* Use `ml_kmeans()` to cluster `prccd`, `prchd`, `prcld` of "BRITISH AMER TOBACCO PLC".

```{r clustering1}

# 1. select subset from "BRITISH AMER TOBACCO PLC"


# 2. Vectorize the features using `ft_vector_assembler` and `ft_standard_scaler`


# 3. Assemble features (K-Means needs a vector column) of prccd, prchd and prcld

# 4. K-Means (example with k=5)
kmeans_model 

results <- ml_transform(kmeans_model, returns_featured_tbl)


```

### Exercise 2.9 Correlation of close price

* Use `ml_corr` to work out correlation of `prccd` from "BRITISH AMER TOBACCO PLC" and "GOODWIN PLC".

```{r correlation}
# 1. Select "BRITISH AMER TOBACCO PLC" and "GOODWIN PLC", both iids are "01W".
GOODWIN_BAT_01W 

# 2. Widen the data using `sdf_pivot()` to achieve rows as company name and their prccd price


# 3. (optional) Rename the rows


# 4. Calculate correlation

```

## Part 2 ends.

